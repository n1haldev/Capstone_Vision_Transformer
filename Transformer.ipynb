{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd3k3cSF_Fgw",
        "outputId": "925249d3-a1e4-4301-cbe8-22d27564d818"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-1.0.8-py3-none-any.whl.metadata (53 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.5)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->timm)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->timm)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->timm)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->timm)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->timm)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->timm)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->timm)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->timm)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->timm)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->timm)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->timm)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->timm)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Downloading timm-1.0.8-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 timm-1.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install timm\n",
        "from timm.models.layers import to_2tuple, trunc_normal_, DropPath\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yvPGTdM_7Gw",
        "outputId": "b5a07a43-689b-4453-956d-b203a10a944c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing with patch size: 24\n",
            "Input shape: torch.Size([1, 3, 226, 226])\n",
            "Padded shape: torch.Size([1, 3, 240, 240])\n",
            "Final output shape for patch size 24: torch.Size([1, 768, 40, 40])\n",
            "\n",
            "Testing with patch size: 12\n",
            "Input shape: torch.Size([1, 3, 226, 226])\n",
            "Padded shape: torch.Size([1, 3, 228, 228])\n",
            "Final output shape for patch size 12: torch.Size([1, 768, 29, 29])\n",
            "\n",
            "Testing with patch size: 4\n",
            "Input shape: torch.Size([1, 3, 226, 226])\n",
            "Padded shape: torch.Size([1, 3, 228, 228])\n",
            "Final output shape for patch size 4: torch.Size([1, 768, 57, 57])\n"
          ]
        }
      ],
      "source": [
        "from timm.models.layers import to_2tuple, trunc_normal_, DropPath\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chan=3, embed_dim=768, multi_conv=False):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        if multi_conv:\n",
        "            if patch_size[0] == 24:\n",
        "                self.proj = nn.Sequential(\n",
        "                    nn.Conv2d(in_chan, embed_dim // 4, kernel_size=7, stride=3, padding=2),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "            elif patch_size[0] == 12:\n",
        "                self.proj = nn.Sequential(\n",
        "                    nn.Conv2d(in_chan, embed_dim // 4, kernel_size=7, stride=4, padding=3),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "            elif patch_size[0] == 4:\n",
        "                self.proj = nn.Sequential(\n",
        "                    nn.Conv2d(in_chan, embed_dim // 4, kernel_size=7, stride=2, padding=3),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported patch size {patch_size[0]}\")\n",
        "        else:\n",
        "            self.proj = nn.Conv2d(in_chan, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x, extra_padding=False):\n",
        "        B, C, H, W = x.shape\n",
        "        print(f\"Input shape: {x.shape}\")\n",
        "        if extra_padding and (H % self.patch_size[0] != 0 or W % self.patch_size[1] != 0):\n",
        "            p_l = (self.patch_size[1] - W % self.patch_size[1]) // 2\n",
        "            p_r = (self.patch_size[1] - W % self.patch_size[1]) - p_l\n",
        "            p_t = (self.patch_size[0] - H % self.patch_size[0]) // 2\n",
        "            p_b = (self.patch_size[0] - H % self.patch_size[0]) - p_t\n",
        "            x = F.pad(x, (p_l, p_r, p_t, p_b))\n",
        "            print(f\"Padded shape: {x.shape}\")\n",
        "\n",
        "        for i, layer in enumerate(self.proj):\n",
        "            x = layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# to run the code with example random tensor\n",
        "img_size = 226\n",
        "patch_sizes = [24, 12, 4]\n",
        "in_chan = 3\n",
        "embed_dim = 768\n",
        "\n",
        "for patch_size in patch_sizes:\n",
        "    print(f\"\\nTesting with patch size: {patch_size}\")\n",
        "    model = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chan=in_chan, embed_dim=embed_dim, multi_conv=True)\n",
        "    x = torch.randn(1, in_chan, img_size, img_size)  # Example input tensor\n",
        "    out = model(x, extra_padding=True)\n",
        "    print(f\"Final output shape for patch size {patch_size}: {out.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMz-vKNKPwCf"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., patch_size=16):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.wq = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.wk = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.wv = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        if self.patch_size == 24:\n",
        "            q = self.wq(x[:, 0:1, ...]).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)  \n",
        "        elif self.patch_size == 12:\n",
        "            q = self.wq(x[:, 0:1, ...]).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)  \n",
        "        else:\n",
        "            q = self.wq(x[:, 0:1, ...]).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)  \n",
        "\n",
        "        k = self.wk(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)  \n",
        "        v = self.wv(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)  \n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale  \n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, 1, C) \n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htj9qyRcgUwc"
      },
      "outputs": [],
      "source": [
        "class CrossAttentionBlock(nn.Module):\n",
        "  def __init__(self,dim,num_heads,mlp_ratio=4.,qkv_bias=False,qk_scale = None, drop = 0. , attn_drop = 0.,\n",
        "               drop_path = 0. , act_layer = nn.GELU , norm_layer = nn.LayerNorm , has_mlp = True , patch_size=16):\n",
        "     super().__init__()\n",
        "     self.norm1 = norm_layer(dim)\n",
        "     self.attn = CrossAttention(dim ,num_heads = num_heads , qkv_bias = qkv_bias, qk_scale = qk_scale , attn_drop = attn_drop , proj_drop = drop , patch_size = patch_size)\n",
        "     self.drop_path = DropPath(drop_path) if drop_path>0. else nn.Identity()\n",
        "     self.has_mlp = has_mlp\n",
        "     if self.has_mlp:\n",
        "      self.norm2 = norm_layer(dim)\n",
        "      self.mlp = Mlp(in_features = dim ,hidden_features = mlp_hidden_dim , act_layer = act_layer , drop = drop)\n",
        "\n",
        "\n",
        "  def forward(self , x):\n",
        "    x = x[:, 0:1, ...]+self.drop_path(self.attn(self.norm(x)))\n",
        "    if self.has_mlp:\n",
        "      x= x+self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "b_wwMJRQEyQ0",
        "outputId": "d4248d6b-26ac-4f70-9918-e79e1cb5d4bc"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-3-e87c552f97d9>, line 26)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-e87c552f97d9>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "class MultiScaleBlock(nn.Module):\n",
        "  def __init__(self,dim,patches , depth , num_heads ,mlp_ratio , act_layer=nn.GELU, qkv_bias=False ,qk_scale = None, attn_drop = 0. , drop=0. ,norm_layer = nn.LayerNorm):\n",
        "    super().__init__()\n",
        "\n",
        "     #creating branches based on the embedding dim\n",
        "    num_branches = len(dim)\n",
        "    self.num_branches  = num_branches\n",
        "\n",
        "    #transformer block for each branch\n",
        "    self.blocks = nn.ModuleList()\n",
        "    for d in range(num_branches):\n",
        "      temp = []\n",
        "      for i in range(depth[d]):\n",
        "        temp.append(\n",
        "            Block(dim = dim[d],num_heads = num_heads[d], mlp_ratio = mlp_ratio[d],\n",
        "                 qkv_bias = qkv_bias,drop =drop,attn_drop= attn_drop,drop_path = drop_path[i],norm_layer = norm_layer)\n",
        "        )\n",
        "      if len(temp)!=0:\n",
        "        self.blocks.append(nn.Sequential(*temp))\n",
        "    if len(self.blocks) ==0:\n",
        "      self.blocks=None\n",
        "\n",
        "    #making sure that all the branches are of same size if so creats the projection layers\n",
        "    self.proje = nn.ModuleList()\n",
        "    for d in range(num_branches):\n",
        "      temp = [norm_layer(dim[d]),act_layer(),nn.Linear(dim[d],dim[(d+1)%num_branches])]\n",
        "    self.proje.append(nn.Sequential(*temp))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFexfze8RTfX",
        "outputId": "d8302dbb-2e0a-47a1-a776-a6b42beb32da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing with patch size: 24\n",
            "Input shape: torch.Size([1, 3, 224, 224])\n",
            "Padded shape: torch.Size([1, 3, 240, 240])\n",
            "Patch embeddings shape: torch.Size([1, 768, 40, 40])\n",
            "Attention output shape: torch.Size([1, 1, 768])\n",
            "\n",
            "Testing with patch size: 12\n",
            "Input shape: torch.Size([1, 3, 224, 224])\n",
            "Padded shape: torch.Size([1, 3, 228, 228])\n",
            "Patch embeddings shape: torch.Size([1, 768, 29, 29])\n",
            "Attention output shape: torch.Size([1, 1, 768])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "img_size = 224\n",
        "patch_sizes = [24, 12]\n",
        "in_chan = 3\n",
        "embed_dim = 768\n",
        "num_heads = 8\n",
        "\n",
        "for patch_size in patch_sizes:\n",
        "    print(f\"\\nTesting with patch size: {patch_size}\")\n",
        "\n",
        "    patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chan=in_chan, embed_dim=embed_dim, multi_conv=True)\n",
        "\n",
        "    x = torch.randn(1, in_chan, img_size, img_size)\n",
        "\n",
        "    patch_embeddings = patch_embed(x, extra_padding=True)\n",
        "    print(f\"Patch embeddings shape: {patch_embeddings.shape}\")\n",
        "\n",
        "    # Reshape the output of PatchEmbed to match the expected input shape for CrossAttention\n",
        "    # The output shape from PatchEmbed (B, embed_dim, H', W')\n",
        "    #flatten the spatial dimensions to get the shape (B, N, C) for CrossAttention\n",
        "    B, C, H_prime, W_prime = patch_embeddings.shape\n",
        "    N = (H_prime * W_prime)  # Number of patches\n",
        "    patch_embeddings = patch_embeddings.permute(0, 2, 3, 1).reshape(B, N, C)  \n",
        "\n",
        "    cross_attention = CrossAttention(dim=embed_dim, num_heads=num_heads, patch_size=patch_size)\n",
        "\n",
        "    attention_output = cross_attention(patch_embeddings)\n",
        "    print(f\"Attention output shape: {attention_output.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb20D6FzRqyv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
