{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /Users/naren/miniconda3/lib/python3.12/site-packages (1.0.8)\n",
      "Requirement already satisfied: torch in /Users/naren/miniconda3/lib/python3.12/site-packages (from timm) (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/naren/miniconda3/lib/python3.12/site-packages (from timm) (0.17.2)\n",
      "Requirement already satisfied: pyyaml in /Users/naren/miniconda3/lib/python3.12/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /Users/naren/miniconda3/lib/python3.12/site-packages (from timm) (0.24.3)\n",
      "Requirement already satisfied: safetensors in /Users/naren/miniconda3/lib/python3.12/site-packages (from timm) (0.4.3)\n",
      "Requirement already satisfied: filelock in /Users/naren/miniconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/naren/miniconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/naren/miniconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (23.1)\n",
      "Requirement already satisfied: requests in /Users/naren/miniconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/naren/miniconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/naren/miniconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/naren/miniconda3/lib/python3.12/site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/naren/miniconda3/lib/python3.12/site-packages (from torch->timm) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/naren/miniconda3/lib/python3.12/site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: numpy in /Users/naren/miniconda3/lib/python3.12/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/naren/miniconda3/lib/python3.12/site-packages (from torchvision->timm) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/naren/miniconda3/lib/python3.12/site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/naren/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/naren/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/naren/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/naren/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/naren/miniconda3/lib/python3.12/site-packages (from sympy->torch->timm) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "#Installs the timm library for pre-trained PyTorch models.\n",
    "!pip install timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.layers import to_2tuple, trunc_normal_, DropPath\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chan=3, embed_dim=768, multi_conv=False, use_simple_conv=False):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        if multi_conv and not use_simple_conv:\n",
    "            if patch_size[0] == 24:\n",
    "                self.proj = nn.Sequential(\n",
    "                    nn.Conv2d(in_chan, embed_dim // 4, kernel_size=7, stride=3, padding=2),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "            elif patch_size[0] == 12:\n",
    "                self.proj = nn.Sequential(\n",
    "                    nn.Conv2d(in_chan, embed_dim // 4, kernel_size=7, stride=4, padding=3),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "            elif patch_size[0] == 4:\n",
    "                self.proj = nn.Sequential(\n",
    "                    nn.Conv2d(in_chan, embed_dim // 4, kernel_size=7, stride=2, padding=3),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported patch size {patch_size[0]}\")\n",
    "        elif use_simple_conv:\n",
    "            # Simple convolutional embedding\n",
    "            self.proj = nn.Conv2d(in_chan, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        else:\n",
    "            # Default behavior if multi_conv is not used\n",
    "            self.proj = nn.Conv2d(in_chan, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x, extra_padding=False):\n",
    "        B, C, H, W = x.shape\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        if extra_padding and (H % self.patch_size[0] != 0 or W % self.patch_size[1] != 0):\n",
    "            p_l = (self.patch_size[1] - W % self.patch_size[1]) // 2\n",
    "            p_r = (self.patch_size[1] - W % self.patch_size[1]) - p_l\n",
    "            p_t = (self.patch_size[0] - H % self.patch_size[0]) // 2\n",
    "            p_b = (self.patch_size[0] - H % self.patch_size[0]) - p_t\n",
    "            x = F.pad(x, (p_l, p_r, p_t, p_b))\n",
    "            print(f\"Padded shape: {x.shape}\")\n",
    "\n",
    "        x = self.proj(x)  # Apply the projection (whether multi-conv or simple-conv)\n",
    "        x = x.flatten(2).transpose(1, 2)  # Flatten and transpose for transformer input\n",
    "\n",
    "        return x\n",
    "\n",
    "# Utility function to convert values to tuple if they aren't already\n",
    "def to_2tuple(x):\n",
    "    if isinstance(x, tuple):\n",
    "        return x\n",
    "    return (x, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Utility function to create N identical layers\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Construct a layernorm module.\"\"\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"A residual connection followed by a layer norm.\"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Encoder is made up of self-attn and feed forward (defined below)\"\"\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Core encoder is a stack of N layers\"\"\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class Sparsifier(nn.Module):\n",
    "    \"\"\"Applies sparsification to the encoder output.\"\"\"\n",
    "    def __init__(self, threshold=0.1):\n",
    "        super(Sparsifier, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (torch.abs(x) > self.threshold).float()\n",
    "        return x * mask\n",
    "\n",
    "# Assuming you already have the PatchEmbed class from your previous code\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=(224, 224), patch_size=(16, 24), in_chans=3, num_classes=1000,\n",
    "                 embed_dim=[384, 768], depth=([1, 4, 0], [1, 4, 0], [1, 4, 0]), num_heads=[12, 12],\n",
    "                 mlp_ratio=[4, 4, 1], qkv_bias=False, qk_scale=None, drop_rate=0.,\n",
    "                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm, sparsify=False, multi_conv=False):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size if isinstance(img_size, list) else to_2tuple(img_size)\n",
    "        self.num_branches = len(patch_size)\n",
    "\n",
    "        self.patch_embed = nn.ModuleList([PatchEmbed(img_size=im_s, patch_size=p, in_chan=in_chans,\n",
    "                                                     embed_dim=d, multi_conv=multi_conv)\n",
    "                                          for im_s, p, d in zip(img_size, patch_size, embed_dim)])\n",
    "\n",
    "        self.pos_embed = nn.ParameterList([nn.Parameter(torch.zeros(1, 1 + _compute_num_patches(im_s, p), d))\n",
    "                                           for im_s, p, d in zip(img_size, patch_size, embed_dim)])\n",
    "\n",
    "        self.cls_token = nn.ParameterList([nn.Parameter(torch.zeros(1, 1, d)) for d in embed_dim])\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # Create encoder layers\n",
    "        self.encoders = nn.ModuleList([Encoder(EncoderLayer(embed_dim[i], MultiHeadedAttention(num_heads[i], embed_dim[i]),\n",
    "                                                     nn.Sequential(nn.Linear(embed_dim[i], embed_dim[i] * mlp_ratio[i]),\n",
    "                                                                   nn.ReLU(),\n",
    "                                                                   nn.Linear(embed_dim[i] * mlp_ratio[i], embed_dim[i])),\n",
    "                                                     drop_rate), depth[i][-1])\n",
    "                                       for i in range(self.num_branches)])\n",
    "        \n",
    "        # Sparsifier module for each branch\n",
    "        self.sparsifiers = nn.ModuleList([Sparsifier() if sparsify else nn.Identity() for _ in range(self.num_branches)])\n",
    "\n",
    "        self.norm = nn.ModuleList([norm_layer(d) for d in embed_dim])\n",
    "        self.head = nn.ModuleList([nn.Linear(d, num_classes) if num_classes > 0 else nn.Identity() for d in embed_dim])\n",
    "\n",
    "        for i in range(self.num_branches):\n",
    "            trunc_normal_(self.pos_embed[i], std=.02)\n",
    "            trunc_normal_(self.cls_token[i], std=.02)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        xs = []\n",
    "        for i in range(self.num_branches):\n",
    "            x_ = torch.nn.functional.interpolate(x, size=(self.img_size[i], self.img_size[i]), mode='bicubic') if H != self.img_size[i] else x\n",
    "            tmp = self.patch_embed[i](x_)\n",
    "            cls_tokens = self.cls_token[i].expand(B, -1, -1)\n",
    "            tmp = torch.cat((cls_tokens, tmp), dim=1)\n",
    "            tmp = tmp + self.pos_embed[i]\n",
    "            tmp = self.pos_drop(tmp)\n",
    "            tmp = self.encoders[i](tmp, None)  # Encoder processing\n",
    "            tmp = self.sparsifiers[i](tmp)  # Apply sparsification\n",
    "            xs.append(tmp)\n",
    "\n",
    "        xs = [self.norm[i](x) for i, x in enumerate(xs)]\n",
    "        out = [x[:, 0] for x in xs]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = self.forward_features(x)\n",
    "        ce_logits = [self.head[i](x) for i, x in enumerate(xs)]\n",
    "        ce_logits = torch.mean(torch.stack(ce_logits, dim=0), dim=0)\n",
    "        return ce_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., patch_size=16):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.wq = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.wk = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.wv = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Compute Q, K, V matrices\n",
    "        q = self.wq(x[:, 0:1, ...]).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        k = self.wk(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        v = self.wv(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # Compute attention output\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, 1, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, has_mlp=True, patch_size=16):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = CrossAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                   attn_drop=attn_drop, proj_drop=drop, patch_size=patch_size)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.has_mlp = has_mlp\n",
    "        if self.has_mlp:\n",
    "            self.norm2 = norm_layer(dim)\n",
    "            mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:, 0:1, ...] + self.drop_path(self.attn(self.norm1(x)))\n",
    "        if self.has_mlp:\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleBlock(nn.Module):\n",
    "    def __init__(self, dim, patches, depth, num_heads, mlp_ratio, act_layer=nn.GELU, qkv_bias=False, \n",
    "                 qk_scale=None, attn_drop=0., drop=0., norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "\n",
    "        # Creating branches based on the embedding dimension\n",
    "        num_branches = len(dim)\n",
    "        self.num_branches = num_branches\n",
    "\n",
    "        # Transformer block for each branch\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for d in range(num_branches):\n",
    "            temp = []\n",
    "            for i in range(depth[d]):\n",
    "                temp.append(\n",
    "                    Block(dim=dim[d], num_heads=num_heads[d], mlp_ratio=mlp_ratio[d],\n",
    "                          qkv_bias=qkv_bias, drop=drop, attn_drop=attn_drop, \n",
    "                          drop_path=drop_path[i], norm_layer=norm_layer)\n",
    "                )\n",
    "            if len(temp) != 0:\n",
    "                self.blocks.append(nn.Sequential(*temp))\n",
    "        if len(self.blocks) == 0:\n",
    "            self.blocks = None\n",
    "\n",
    "        # Ensuring that all branches are of the same size, creating projection layers if so\n",
    "        self.proje = nn.ModuleList()\n",
    "        for d in range(num_branches):\n",
    "            temp = [norm_layer(dim[d]), act_layer(), nn.Linear(dim[d], dim[(d+1) % num_branches])]\n",
    "            self.proje.append(nn.Sequential(*temp))\n",
    "\n",
    "        self.fusion = nn.ModuleList()\n",
    "        for d in range(num_branches):\n",
    "            d_ = d + 1 % (num_branches)\n",
    "            nh = num_heads[d_]\n",
    "            if depth[-1] == 0:\n",
    "                self.fusion.append(CrossAttentionBlock(dim=dim[d_], num_heads=nh, mlp_ratio=mlp_ratio[d],\n",
    "                                                       qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop, \n",
    "                                                       attn_drop=attn_drop, drop_path=drop_path[-1], \n",
    "                                                       norm_layer=norm_layer, has_mlp=False, \n",
    "                                                       patch_size=patches[d_]))\n",
    "            else:\n",
    "                temp = []\n",
    "                for _ in range(depth[-1]):\n",
    "                    temp.append(CrossAttentionBlock(dim=dim[d_], num_heads=nh, mlp_ratio=mlp_ratio[d], \n",
    "                                                    qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop, \n",
    "                                                    attn_drop=attn_drop, drop_path=drop_path[-1], \n",
    "                                                    norm_layer=norm_layer, has_mlp=False, \n",
    "                                                    patch_size=patches[d_]))\n",
    "                self.fusion.append(nn.Sequential(*temp))\n",
    "\n",
    "        self.revert_projs = nn.ModuleList()\n",
    "        for d in range(num_branches):\n",
    "            temp = [norm_layer(dim[(d+1) % num_branches]), act_layer(), \n",
    "                    nn.Linear(dim[(d+1) % num_branches], dim[d])]\n",
    "            self.revert_projs.append(nn.Sequential(*temp))\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs_b = [blocks(x_) for x_, blocks in zip(x, self.blocks)]\n",
    "        proj_cls_tok = [proj(x[:, 0:1]) for x, proj in zip(outs_b, self.proje)]\n",
    "\n",
    "        outs = []\n",
    "        for i in range(self.num_branches):\n",
    "            temp = torch.cat((proj_cls_tok[i], outs_b[(i + 1) % self.num_branches][:, 1:, ...]), dim=1)\n",
    "            temp = self.fusion[i](temp)\n",
    "            reverted_pro_cls_tok = self.revert_projs[i](temp[:, 0:1, ...])\n",
    "            temp = torch.cat((reverted_pro_cls_tok, outs_b[i][:, 0:1, ...]), dim=1)\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels: ['ash', 'beech', 'cattail', 'cedar', 'clover', 'cyprus', 'daisy', 'dandelion', 'dogwood', 'elm', 'fern', 'fig', 'fir', 'juniper', 'maple', 'poison_ivy', 'sweetgum', 'sycamore', 'trout_lily', 'tulip_tree']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Define the paths to your datasets\n",
    "data_dir = '/Users/naren/vit/jetson-inference/python/training/classification/data/PlantCLEF_Subset'  # Directory containing 'train', 'val', and 'test' folders\n",
    "\n",
    "# Extract class labels from the 'train' folder\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "classes = sorted(entry.name for entry in os.scandir(train_dir) if entry.is_dir())\n",
    "\n",
    "# Define a custom dataset class to handle the test set\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root, fname) for fname in os.listdir(root)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, -1  # return a dummy label since the test set doesn't have labels\n",
    "\n",
    "# Define transformations for training and validation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'val'), transform=val_test_transform)\n",
    "test_dataset = TestDataset(root=os.path.join(data_dir, 'test'), transform=val_test_transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Verify the class labels\n",
    "print(\"Class labels:\", train_dataset.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Block class\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=attn_drop)\n",
    "        self.drop_path = nn.Identity() if drop_path == 0 else nn.Dropout(drop_path)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            act_layer(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0])\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "# Define the MultiScaleBlock class\n",
    "class MultiScaleBlock(nn.Module):\n",
    "    def __init__(self, dim, patches, depth, num_heads, mlp_ratio, act_layer=nn.GELU, qkv_bias=False, qk_scale=None, attn_drop=0., drop=0., drop_path=None, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        temp = []\n",
    "        for d in range(len(dim)):\n",
    "            for i in range(depth[d]):\n",
    "                temp.append(\n",
    "                    Block(\n",
    "                        dim=dim[d],\n",
    "                        num_heads=num_heads[d],\n",
    "                        mlp_ratio=mlp_ratio[d],\n",
    "                        qkv_bias=qkv_bias,\n",
    "                        drop=drop,\n",
    "                        attn_drop=attn_drop,\n",
    "                        drop_path=drop_path[i],  # Ensure drop_path is indexed correctly\n",
    "                        norm_layer=norm_layer\n",
    "                    )\n",
    "                )\n",
    "            if len(temp) != 0:\n",
    "                self.blocks.append(nn.Sequential(*temp))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return x\n",
    "\n",
    "# Define the Vision Transformer model with classification head\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_chans, num_classes, embed_dim, depth, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm, multi_conv=False):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Flatten the depth list of lists\n",
    "        flat_depth = [item for sublist in depth for item in sublist]\n",
    "\n",
    "        # Calculate dpr based on the flattened depth list\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(flat_depth))]\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MultiScaleBlock(\n",
    "                dim=embed_dim,\n",
    "                patches=patch_size,\n",
    "                depth=block_cfg,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr,  # Pass the drop path here\n",
    "                norm_layer=norm_layer\n",
    "            )\n",
    "            for block_cfg in depth\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.ModuleList([norm_layer(d) for d in embed_dim])\n",
    "        self.head = nn.ModuleList([nn.Linear(d, num_classes) if num_classes > 0 else nn.Identity() for d in embed_dim])\n",
    "\n",
    "        for i in range(len(embed_dim)):\n",
    "            trunc_normal_(self.pos_embed[i], std=.02)\n",
    "            trunc_normal_(self.cls_token[i], std=.02)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        xs = []\n",
    "        for i in range(len(self.embed_dim)):\n",
    "            x_ = torch.nn.functional.interpolate(x, size=(self.img_size[i], self.img_size[i]), mode='bicubic') if H != self.img_size[i] else x\n",
    "            tmp = self.patch_embed[i](x_)\n",
    "            cls_tokens = self.cls_token[i].expand(B, -1, -1)\n",
    "            tmp = torch.cat((cls_tokens, tmp), dim=1)\n",
    "            tmp = tmp + self.pos_embed[i]\n",
    "            tmp = self.pos_drop(tmp)\n",
    "            xs.append(tmp)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            xs = blk(xs)\n",
    "\n",
    "        xs = [self.norm[i](x) for i, x in enumerate(xs)]\n",
    "        out = [x[:, 0] for x in xs]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = self.forward_features(x)\n",
    "        ce_logits = [self.head[i](x) for i, x in enumerate(xs)]\n",
    "        ce_logits = torch.mean(torch.stack(ce_logits, dim=0), dim=0)\n",
    "        return ce_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train one epoch\n",
    "def train_one_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader.dataset)\n",
    "    epoch_accuracy = correct / total\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "# Function to validate the model\n",
    "def validate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with patch size: 24\n",
      "Input shape: torch.Size([1, 3, 224, 224])\n",
      "Input shape: torch.Size([1, 3, 224, 224])\n",
      "Padded shape: torch.Size([1, 3, 240, 240])\n",
      "Patch embeddings raw shape: torch.Size([1, 1600, 768])\n",
      "Padded shape: torch.Size([1, 3, 224, 224]) (no further reshaping required)\n",
      "Patch embeddings shape: torch.Size([1, 1600, 768])\n",
      "Attention output shape: torch.Size([1, 1, 768])\n",
      "\n",
      "Testing with patch size: 12\n",
      "Input shape: torch.Size([1, 3, 224, 224])\n",
      "Input shape: torch.Size([1, 3, 224, 224])\n",
      "Padded shape: torch.Size([1, 3, 228, 228])\n",
      "Patch embeddings raw shape: torch.Size([1, 841, 768])\n",
      "Padded shape: torch.Size([1, 3, 224, 224]) (no further reshaping required)\n",
      "Patch embeddings shape: torch.Size([1, 841, 768])\n",
      "Attention output shape: torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "img_size = 224\n",
    "patch_sizes = [24, 12, 4]\n",
    "in_chan = 3\n",
    "embed_dim = 768\n",
    "num_heads = 8\n",
    "\n",
    "for patch_size in patch_sizes:\n",
    "    print(f\"\\nTesting with patch size: {patch_size}\")\n",
    "\n",
    "    # Initialize the PatchEmbed layer\n",
    "    patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chan=in_chan, embed_dim=embed_dim, multi_conv=True)\n",
    "\n",
    "    # Generate a random input tensor simulating an image\n",
    "    x = torch.randn(1, in_chan, img_size, img_size)\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "    # Generate patch embeddings and pad if necessary\n",
    "    patch_embeddings = patch_embed(x, extra_padding=True)\n",
    "\n",
    "    # Check the shape of the patch_embeddings tensor\n",
    "    print(f\"Patch embeddings raw shape: {patch_embeddings.shape}\")\n",
    "\n",
    "    if patch_embeddings.ndimension() == 4:  # If it has the shape (B, C, H', W')\n",
    "        B, C, H_prime, W_prime = patch_embeddings.shape\n",
    "        print(f\"Padded shape: {x.shape}\")\n",
    "        print(f\"Patch embeddings shape: {patch_embeddings.shape}\")\n",
    "\n",
    "        # Reshape for CrossAttention input\n",
    "        N = H_prime * W_prime  # Number of patches\n",
    "        patch_embeddings = patch_embeddings.permute(0, 2, 3, 1).reshape(B, N, C)  # (B, H', W', C) -> (B, N, C)\n",
    "    elif patch_embeddings.ndimension() == 3:  # If it has already been flattened to (B, N, C)\n",
    "        B, N, C = patch_embeddings.shape\n",
    "        print(f\"Padded shape: {x.shape} (no further reshaping required)\")\n",
    "        print(f\"Patch embeddings shape: {patch_embeddings.shape}\")\n",
    "\n",
    "    # Initialize and apply the CrossAttention layer\n",
    "    cross_attention = CrossAttention(dim=embed_dim, num_heads=num_heads, patch_size=patch_size)\n",
    "    attention_output = cross_attention(patch_embeddings)\n",
    "    print(f\"Attention output shape: {attention_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
