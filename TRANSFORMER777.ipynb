{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /Users/naren/miniconda3/lib/python3.12/site-packages (1.0.8)\n",
      "Requirement already satisfied: torch in /Users/naren/miniconda3/lib/python3.12/site-packages (from timm) (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/naren/miniconda3/lib/python3.12/site-packages (from timm) (0.17.2)\n",
      "Requirement already satisfied: pyyaml in /Users/naren/miniconda3/lib/python3.12/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /Users/naren/miniconda3/lib/python3.12/site-packages (from timm) (0.24.3)\n",
      "Requirement already satisfied: safetensors in /Users/naren/miniconda3/lib/python3.12/site-packages (from timm) (0.4.3)\n",
      "Requirement already satisfied: filelock in /Users/naren/miniconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/naren/miniconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/naren/miniconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (23.1)\n",
      "Requirement already satisfied: requests in /Users/naren/miniconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/naren/miniconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/naren/miniconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/naren/miniconda3/lib/python3.12/site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/naren/miniconda3/lib/python3.12/site-packages (from torch->timm) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/naren/miniconda3/lib/python3.12/site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: numpy in /Users/naren/miniconda3/lib/python3.12/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/naren/miniconda3/lib/python3.12/site-packages (from torchvision->timm) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/naren/miniconda3/lib/python3.12/site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/naren/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/naren/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/naren/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/naren/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/naren/miniconda3/lib/python3.12/site-packages (from sympy->torch->timm) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "#Installs the timm library for pre-trained PyTorch models.\n",
    "!pip install timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from timm.models.layers import to_2tuple, trunc_normal_, DropPath\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chan=3, embed_dim=768, multi_conv=False, use_simple_conv=False):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        if multi_conv and not use_simple_conv:\n",
    "            if patch_size[0] == 24:\n",
    "                self.proj = nn.Sequential(\n",
    "                    nn.Conv2d(in_chan, embed_dim // 4, kernel_size=7, stride=3, padding=2),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "            elif patch_size[0] == 12:\n",
    "                self.proj = nn.Sequential(\n",
    "                    nn.Conv2d(in_chan, embed_dim // 4, kernel_size=7, stride=4, padding=3),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "            elif patch_size[0] == 4:\n",
    "                self.proj = nn.Sequential(\n",
    "                    nn.Conv2d(in_chan, embed_dim // 4, kernel_size=7, stride=2, padding=3),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported patch size {patch_size[0]}\")\n",
    "        elif use_simple_conv:\n",
    "            # Simple convolutional embedding\n",
    "            self.proj = nn.Conv2d(in_chan, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        else:\n",
    "            # Default behavior if multi_conv is not used\n",
    "            self.proj = nn.Conv2d(in_chan, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x, extra_padding=False):\n",
    "        B, C, H, W = x.shape\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        if extra_padding and (H % self.patch_size[0] != 0 or W % self.patch_size[1] != 0):\n",
    "            p_l = (self.patch_size[1] - W % self.patch_size[1]) // 2\n",
    "            p_r = (self.patch_size[1] - W % self.patch_size[1]) - p_l\n",
    "            p_t = (self.patch_size[0] - H % self.patch_size[0]) // 2\n",
    "            p_b = (self.patch_size[0] - H % self.patch_size[0]) - p_t\n",
    "            x = F.pad(x, (p_l, p_r, p_t, p_b))\n",
    "            print(f\"Padded shape: {x.shape}\")\n",
    "\n",
    "        x = self.proj(x)  # Apply the projection (whether multi-conv or simple-conv)\n",
    "        x = x.flatten(2).transpose(1, 2)  # Flatten and transpose for transformer input\n",
    "\n",
    "        return x\n",
    "\n",
    "# Utility function to convert values to tuple if they aren't already\n",
    "def to_2tuple(x):\n",
    "    if isinstance(x, tuple):\n",
    "        return x\n",
    "    return (x, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Utility function to create N identical layers\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Construct a layernorm module.\"\"\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"A residual connection followed by a layer norm.\"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Encoder is made up of self-attn and feed forward (defined below)\"\"\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Core encoder is a stack of N layers\"\"\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class Sparsifier(nn.Module):\n",
    "    \"\"\"Applies sparsification to the encoder output.\"\"\"\n",
    "    def __init__(self, threshold=0.1):\n",
    "        super(Sparsifier, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (torch.abs(x) > self.threshold).float()\n",
    "        return x * mask\n",
    "\n",
    "# Assuming you already have the PatchEmbed class from your previous code\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=(224, 224), patch_size=(16, 24), in_chans=3, num_classes=1000,\n",
    "                 embed_dim=[384, 768], depth=([1, 4, 0], [1, 4, 0], [1, 4, 0]), num_heads=[12, 12],\n",
    "                 mlp_ratio=[4, 4, 1], qkv_bias=False, qk_scale=None, drop_rate=0.,\n",
    "                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm, sparsify=False, multi_conv=False):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size if isinstance(img_size, list) else to_2tuple(img_size)\n",
    "        self.num_branches = len(patch_size)\n",
    "\n",
    "        self.patch_embed = nn.ModuleList([PatchEmbed(img_size=im_s, patch_size=p, in_chan=in_chans,\n",
    "                                                     embed_dim=d, multi_conv=multi_conv)\n",
    "                                          for im_s, p, d in zip(img_size, patch_size, embed_dim)])\n",
    "\n",
    "        self.pos_embed = nn.ParameterList([nn.Parameter(torch.zeros(1, 1 + _compute_num_patches(im_s, p), d))\n",
    "                                           for im_s, p, d in zip(img_size, patch_size, embed_dim)])\n",
    "\n",
    "        self.cls_token = nn.ParameterList([nn.Parameter(torch.zeros(1, 1, d)) for d in embed_dim])\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # Create encoder layers\n",
    "        self.encoders = nn.ModuleList([Encoder(EncoderLayer(embed_dim[i], MultiHeadedAttention(num_heads[i], embed_dim[i]),\n",
    "                                                     nn.Sequential(nn.Linear(embed_dim[i], embed_dim[i] * mlp_ratio[i]),\n",
    "                                                                   nn.ReLU(),\n",
    "                                                                   nn.Linear(embed_dim[i] * mlp_ratio[i], embed_dim[i])),\n",
    "                                                     drop_rate), depth[i][-1])\n",
    "                                       for i in range(self.num_branches)])\n",
    "        \n",
    "        # Sparsifier module for each branch\n",
    "        self.sparsifiers = nn.ModuleList([Sparsifier() if sparsify else nn.Identity() for _ in range(self.num_branches)])\n",
    "\n",
    "        self.norm = nn.ModuleList([norm_layer(d) for d in embed_dim])\n",
    "        self.head = nn.ModuleList([nn.Linear(d, num_classes) if num_classes > 0 else nn.Identity() for d in embed_dim])\n",
    "\n",
    "        for i in range(self.num_branches):\n",
    "            trunc_normal_(self.pos_embed[i], std=.02)\n",
    "            trunc_normal_(self.cls_token[i], std=.02)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        xs = []\n",
    "        for i in range(self.num_branches):\n",
    "            x_ = torch.nn.functional.interpolate(x, size=(self.img_size[i], self.img_size[i]), mode='bicubic') if H != self.img_size[i] else x\n",
    "            tmp = self.patch_embed[i](x_)\n",
    "            cls_tokens = self.cls_token[i].expand(B, -1, -1)\n",
    "            tmp = torch.cat((cls_tokens, tmp), dim=1)\n",
    "            tmp = tmp + self.pos_embed[i]\n",
    "            tmp = self.pos_drop(tmp)\n",
    "            tmp = self.encoders[i](tmp, None)  # Encoder processing\n",
    "            tmp = self.sparsifiers[i](tmp)  # Apply sparsification\n",
    "            xs.append(tmp)\n",
    "\n",
    "        xs = [self.norm[i](x) for i, x in enumerate(xs)]\n",
    "        out = [x[:, 0] for x in xs]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = self.forward_features(x)\n",
    "        ce_logits = [self.head[i](x) for i, x in enumerate(xs)]\n",
    "        ce_logits = torch.mean(torch.stack(ce_logits, dim=0), dim=0)\n",
    "        return ce_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., patch_size=16):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.wq = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.wk = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.wv = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Compute Q, K, V matrices\n",
    "        q = self.wq(x[:, 0:1, ...]).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        k = self.wk(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        v = self.wv(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # Compute attention output\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, 1, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, has_mlp=True, patch_size=16):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = CrossAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                   attn_drop=attn_drop, proj_drop=drop, patch_size=patch_size)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.has_mlp = has_mlp\n",
    "        if self.has_mlp:\n",
    "            self.norm2 = norm_layer(dim)\n",
    "            mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:, 0:1, ...] + self.drop_path(self.attn(self.norm1(x)))\n",
    "        if self.has_mlp:\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleBlock(nn.Module):\n",
    "    def __init__(self, dim, patches, depth, num_heads, mlp_ratio, act_layer=nn.GELU, qkv_bias=False, \n",
    "                 qk_scale=None, attn_drop=0., drop=0., norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "\n",
    "        # Creating branches based on the embedding dimension\n",
    "        num_branches = len(dim)\n",
    "        self.num_branches = num_branches\n",
    "\n",
    "        # Transformer block for each branch\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for d in range(num_branches):\n",
    "            temp = []\n",
    "            for i in range(depth[d]):\n",
    "                temp.append(\n",
    "                    Block(dim=dim[d], num_heads=num_heads[d], mlp_ratio=mlp_ratio[d],\n",
    "                          qkv_bias=qkv_bias, drop=drop, attn_drop=attn_drop, \n",
    "                          drop_path=drop_path[i], norm_layer=norm_layer)\n",
    "                )\n",
    "            if len(temp) != 0:\n",
    "                self.blocks.append(nn.Sequential(*temp))\n",
    "        if len(self.blocks) == 0:\n",
    "            self.blocks = None\n",
    "\n",
    "        # Ensuring that all branches are of the same size, creating projection layers if so\n",
    "        self.proje = nn.ModuleList()\n",
    "        for d in range(num_branches):\n",
    "            temp = [norm_layer(dim[d]), act_layer(), nn.Linear(dim[d], dim[(d+1) % num_branches])]\n",
    "            self.proje.append(nn.Sequential(*temp))\n",
    "\n",
    "        self.fusion = nn.ModuleList()\n",
    "        for d in range(num_branches):\n",
    "            d_ = d + 1 % (num_branches)\n",
    "            nh = num_heads[d_]\n",
    "            if depth[-1] == 0:\n",
    "                self.fusion.append(CrossAttentionBlock(dim=dim[d_], num_heads=nh, mlp_ratio=mlp_ratio[d],\n",
    "                                                       qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop, \n",
    "                                                       attn_drop=attn_drop, drop_path=drop_path[-1], \n",
    "                                                       norm_layer=norm_layer, has_mlp=False, \n",
    "                                                       patch_size=patches[d_]))\n",
    "            else:\n",
    "                temp = []\n",
    "                for _ in range(depth[-1]):\n",
    "                    temp.append(CrossAttentionBlock(dim=dim[d_], num_heads=nh, mlp_ratio=mlp_ratio[d], \n",
    "                                                    qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop, \n",
    "                                                    attn_drop=attn_drop, drop_path=drop_path[-1], \n",
    "                                                    norm_layer=norm_layer, has_mlp=False, \n",
    "                                                    patch_size=patches[d_]))\n",
    "                self.fusion.append(nn.Sequential(*temp))\n",
    "\n",
    "        self.revert_projs = nn.ModuleList()\n",
    "        for d in range(num_branches):\n",
    "            temp = [norm_layer(dim[(d+1) % num_branches]), act_layer(), \n",
    "                    nn.Linear(dim[(d+1) % num_branches], dim[d])]\n",
    "            self.revert_projs.append(nn.Sequential(*temp))\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs_b = [blocks(x_) for x_, blocks in zip(x, self.blocks)]\n",
    "        proj_cls_tok = [proj(x[:, 0:1]) for x, proj in zip(outs_b, self.proje)]\n",
    "\n",
    "        outs = []\n",
    "        for i in range(self.num_branches):\n",
    "            temp = torch.cat((proj_cls_tok[i], outs_b[(i + 1) % self.num_branches][:, 1:, ...]), dim=1)\n",
    "            temp = self.fusion[i](temp)\n",
    "            reverted_pro_cls_tok = self.revert_projs[i](temp[:, 0:1, ...])\n",
    "            temp = torch.cat((reverted_pro_cls_tok, outs_b[i][:, 0:1, ...]), dim=1)\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mPlantCLEF_Subset\u001b[m\u001b[m/  Transformer.ipynb\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels: ['ash', 'beech', 'cattail', 'cedar', 'clover', 'cyprus', 'daisy', 'dandelion', 'dogwood', 'elm', 'fern', 'fig', 'fir', 'juniper', 'maple', 'poison_ivy', 'sweetgum', 'sycamore', 'trout_lily', 'tulip_tree']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Define the paths to your datasets\n",
    "data_dir = 'PlantCLEF_Subset'  # Directory containing 'train', 'val', and 'test' folders\n",
    "\n",
    "# Extract class labels from the 'train' folder\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "classes = sorted(entry.name for entry in os.scandir(train_dir) if entry.is_dir())\n",
    "\n",
    "# Define a custom dataset class to handle the test set\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root, fname) for fname in os.listdir(root)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, -1  # return a dummy label since the test set doesn't have labels\n",
    "\n",
    "# Define transformations for training and validation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'val'), transform=val_test_transform)\n",
    "test_dataset = TestDataset(root=os.path.join(data_dir, 'test'), transform=val_test_transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Verify the class labels\n",
    "print(\"Class labels:\", train_dataset.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Block class\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=attn_drop)\n",
    "        self.drop_path = nn.Identity() if drop_path == 0 else nn.Dropout(drop_path)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            act_layer(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0])\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "# Define the MultiScaleBlock class\n",
    "class MultiScaleBlock(nn.Module):\n",
    "    def __init__(self, dim, patches, depth, num_heads, mlp_ratio, act_layer=nn.GELU, qkv_bias=False, qk_scale=None, attn_drop=0., drop=0., drop_path=None, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        temp = []\n",
    "        for d in range(len(dim)):\n",
    "            for i in range(depth[d]):\n",
    "                temp.append(\n",
    "                    Block(\n",
    "                        dim=dim[d],\n",
    "                        num_heads=num_heads[d],\n",
    "                        mlp_ratio=mlp_ratio[d],\n",
    "                        qkv_bias=qkv_bias,\n",
    "                        drop=drop,\n",
    "                        attn_drop=attn_drop,\n",
    "                        drop_path=drop_path[i],  # Ensure drop_path is indexed correctly\n",
    "                        norm_layer=norm_layer\n",
    "                    )\n",
    "                )\n",
    "            if len(temp) != 0:\n",
    "                self.blocks.append(nn.Sequential(*temp))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return x\n",
    "\n",
    "# Define the Vision Transformer model with classification head\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_chans, num_classes, embed_dim, depth, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm, multi_conv=False):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Flatten the depth list of lists\n",
    "        flat_depth = [item for sublist in depth for item in sublist]\n",
    "\n",
    "        # Calculate dpr based on the flattened depth list\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(flat_depth))]\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MultiScaleBlock(\n",
    "                dim=embed_dim,\n",
    "                patches=patch_size,\n",
    "                depth=block_cfg,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr,  # Pass the drop path here\n",
    "                norm_layer=norm_layer\n",
    "            )\n",
    "            for block_cfg in depth\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.ModuleList([norm_layer(d) for d in embed_dim])\n",
    "        self.head = nn.ModuleList([nn.Linear(d, num_classes) if num_classes > 0 else nn.Identity() for d in embed_dim])\n",
    "\n",
    "        for i in range(len(embed_dim)):\n",
    "            trunc_normal_(self.pos_embed[i], std=.02)\n",
    "            trunc_normal_(self.cls_token[i], std=.02)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        xs = []\n",
    "        for i in range(len(self.embed_dim)):\n",
    "            x_ = torch.nn.functional.interpolate(x, size=(self.img_size[i], self.img_size[i]), mode='bicubic') if H != self.img_size[i] else x\n",
    "            tmp = self.patch_embed[i](x_)\n",
    "            cls_tokens = self.cls_token[i].expand(B, -1, -1)\n",
    "            tmp = torch.cat((cls_tokens, tmp), dim=1)\n",
    "            tmp = tmp + self.pos_embed[i]\n",
    "            tmp = self.pos_drop(tmp)\n",
    "            xs.append(tmp)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            xs = blk(xs)\n",
    "\n",
    "        xs = [self.norm[i](x) for i, x in enumerate(xs)]\n",
    "        out = [x[:, 0] for x in xs]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = self.forward_features(x)\n",
    "        ce_logits = [self.head[i](x) for i, x in enumerate(xs)]\n",
    "        ce_logits = torch.mean(torch.stack(ce_logits, dim=0), dim=0)\n",
    "        return ce_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train one epoch\n",
    "def train_one_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader.dataset)\n",
    "    epoch_accuracy = correct / total\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "# Function to validate the model\n",
    "def validate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with patch size: 24\n",
      "Input shape: torch.Size([1, 3, 224, 224])\n",
      "Input shape: torch.Size([1, 3, 224, 224])\n",
      "Padded shape: torch.Size([1, 3, 240, 240])\n",
      "Patch embeddings raw shape: torch.Size([1, 1600, 768])\n",
      "Padded shape: torch.Size([1, 3, 224, 224]) (no further reshaping required)\n",
      "Patch embeddings shape: torch.Size([1, 1600, 768])\n",
      "Attention output shape: torch.Size([1, 1, 768])\n",
      "\n",
      "Testing with patch size: 12\n",
      "Input shape: torch.Size([1, 3, 224, 224])\n",
      "Input shape: torch.Size([1, 3, 224, 224])\n",
      "Padded shape: torch.Size([1, 3, 228, 228])\n",
      "Patch embeddings raw shape: torch.Size([1, 841, 768])\n",
      "Padded shape: torch.Size([1, 3, 224, 224]) (no further reshaping required)\n",
      "Patch embeddings shape: torch.Size([1, 841, 768])\n",
      "Attention output shape: torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "img_size = 224\n",
    "patch_sizes = [24, 12]\n",
    "in_chan = 3\n",
    "embed_dim = 768\n",
    "num_heads = 8\n",
    "\n",
    "for patch_size in patch_sizes:\n",
    "    print(f\"\\nTesting with patch size: {patch_size}\")\n",
    "\n",
    "    # Initialize the PatchEmbed layer\n",
    "    patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chan=in_chan, embed_dim=embed_dim, multi_conv=True)\n",
    "\n",
    "    # Generate a random input tensor simulating an image\n",
    "    x = torch.randn(1, in_chan, img_size, img_size)\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "    # Generate patch embeddings and pad if necessary\n",
    "    patch_embeddings = patch_embed(x, extra_padding=True)\n",
    "\n",
    "    # Check the shape of the patch_embeddings tensor\n",
    "    print(f\"Patch embeddings raw shape: {patch_embeddings.shape}\")\n",
    "\n",
    "    if patch_embeddings.ndimension() == 4:  # If it has the shape (B, C, H', W')\n",
    "        B, C, H_prime, W_prime = patch_embeddings.shape\n",
    "        print(f\"Padded shape: {x.shape}\")\n",
    "        print(f\"Patch embeddings shape: {patch_embeddings.shape}\")\n",
    "\n",
    "        # Reshape for CrossAttention input\n",
    "        N = H_prime * W_prime  # Number of patches\n",
    "        patch_embeddings = patch_embeddings.permute(0, 2, 3, 1).reshape(B, N, C)  # (B, H', W', C) -> (B, N, C)\n",
    "    elif patch_embeddings.ndimension() == 3:  # If it has already been flattened to (B, N, C)\n",
    "        B, N, C = patch_embeddings.shape\n",
    "        print(f\"Padded shape: {x.shape} (no further reshaping required)\")\n",
    "        print(f\"Patch embeddings shape: {patch_embeddings.shape}\")\n",
    "\n",
    "    # Initialize and apply the CrossAttention layer\n",
    "    cross_attention = CrossAttention(dim=embed_dim, num_heads=num_heads, patch_size=patch_size)\n",
    "    attention_output = cross_attention(patch_embeddings)\n",
    "    print(f\"Attention output shape: {attention_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m797.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops\n",
      "Successfully installed einops-0.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "class R2LAttentionPlusFFN(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, num_heads, mlp_ratio=1., qkv_bias=False, qk_scale=None,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, drop_path=0., attn_drop=0., drop=0.,\n",
    "                 cls_attn=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if not isinstance(kernel_size, (tuple, list)):\n",
    "            kernel_size = [(kernel_size, kernel_size), (kernel_size, kernel_size), 0]\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        if cls_attn:\n",
    "            self.norm0 = norm_layer(input_channels)\n",
    "        else:\n",
    "            self.norm0 = None\n",
    "\n",
    "        self.norm1 = norm_layer(input_channels)\n",
    "        self.attn = AttentionWithRelPos(\n",
    "            input_channels, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,\n",
    "            attn_map_dim=(kernel_size[0][0], kernel_size[0][1]), num_cls_tokens=1)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(input_channels)\n",
    "        self.mlp = Mlp(in_features=input_channels, hidden_features=int(output_channels * mlp_ratio), out_features=output_channels, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        self.expand = nn.Sequential(\n",
    "            norm_layer(input_channels),\n",
    "            act_layer(),\n",
    "            nn.Linear(input_channels, output_channels)\n",
    "        ) if input_channels != output_channels else None\n",
    "        self.output_channels = output_channels\n",
    "        self.input_channels = input_channels\n",
    "\n",
    "    def forward(self, xs):\n",
    "        # xs is expected to be a tuple of (out, B, H, W, mask, additional_patch_tokens)\n",
    "        out, B, H, W, mask, additional_patch_tokens = xs\n",
    "        \n",
    "        # Process the output from Cross ViT model\n",
    "        cls_tokens = out[:, 0:1, ...]\n",
    "        C = cls_tokens.shape[-1]\n",
    "        cls_tokens = cls_tokens.reshape(B, -1, C)  # (B)x(H/sxW/s)xC\n",
    "\n",
    "        if self.norm0 is not None:\n",
    "            cls_tokens = cls_tokens + self.drop_path(self.attn(self.norm0(cls_tokens)))  # (B)x(H/sxW/s)xC\n",
    "\n",
    "        # Reshape class tokens for concatenation\n",
    "        cls_tokens = cls_tokens.reshape(-1, 1, C)  # (BxH/sxW/s)x1xC\n",
    "\n",
    "        # Concatenate the processed Cross ViT output with additional 4x4 patch tokens\n",
    "        out = torch.cat((cls_tokens, out[:, 1:, ...]), dim=1)\n",
    "\n",
    "        # Add additional 4x4 patch tokens to the output\n",
    "        out = torch.cat((out, additional_patch_tokens), dim=1)\n",
    "        \n",
    "        # Apply attention and feed-forward network on concatenated output\n",
    "        tmp = out\n",
    "        tmp = tmp + self.drop_path(self.attn(self.norm1(tmp), patch_attn=True, mask=mask))\n",
    "        identity = self.expand(tmp) if self.expand is not None else tmp\n",
    "        tmp = identity + self.drop_path(self.mlp(self.norm2(tmp)))\n",
    "\n",
    "        return tmp\n",
    "\n",
    "class AttentionWithRelPos(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.,\n",
    "                 attn_map_dim=None, num_cls_tokens=1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.num_cls_tokens = num_cls_tokens\n",
    "        if attn_map_dim is not None:\n",
    "            one_dim = attn_map_dim[0]\n",
    "            rel_pos_dim = (2 * one_dim - 1)\n",
    "            self.rel_pos = nn.Parameter(torch.zeros(num_heads, rel_pos_dim ** 2))\n",
    "            tmp = torch.arange(rel_pos_dim ** 2).reshape((rel_pos_dim, rel_pos_dim))\n",
    "            out = []\n",
    "            offset_x = offset_y = one_dim // 2\n",
    "            for y in range(one_dim):\n",
    "                for x in range(one_dim):\n",
    "                    for dy in range(one_dim):\n",
    "                        for dx in range(one_dim):\n",
    "                            out.append(tmp[dy - y + offset_y, dx - x + offset_x])\n",
    "            self.rel_pos_index = torch.tensor(out, dtype=torch.long)\n",
    "            trunc_normal_(self.rel_pos, std=.02)\n",
    "        else:\n",
    "            self.rel_pos = None\n",
    "\n",
    "    def forward(self, x, patch_attn=False, mask=None):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        if self.rel_pos is not None and patch_attn:\n",
    "            # Apply relative positional encoding\n",
    "            rel_pos = self.rel_pos[:, self.rel_pos_index.to(attn.device)].reshape(self.num_heads, N - self.num_cls_tokens, N - self.num_cls_tokens)\n",
    "            attn[:, :, self.num_cls_tokens:, self.num_cls_tokens:] = attn[:, :, self.num_cls_tokens:, self.num_cls_tokens:] + rel_pos\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).expand(-1, self.num_heads, -1, -1)\n",
    "            attn = attn.masked_fill(mask == 0, torch.finfo(attn.dtype).min)\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAttBlock(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, num_blocks, num_heads, mlp_ratio=1., qkv_bias=False, qk_scale=None, pool='sc',\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, drop_path_rate=(0.,), attn_drop_rate=0., drop_rate=0.,\n",
    "                 cls_attn=True, peg=False):\n",
    "        super().__init__()\n",
    "        tmp = []\n",
    "        if pool:\n",
    "            tmp.append(Projection(input_channels, output_channels, act_layer=act_layer, mode=pool))\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            kernel_size_ = kernel_size\n",
    "            tmp.append(R2LAttentionPlusFFN(output_channels, output_channels, kernel_size_, num_heads, mlp_ratio, qkv_bias, qk_scale,\n",
    "                                           act_layer=act_layer, norm_layer=norm_layer, drop_path=drop_path_rate[i], attn_drop=attn_drop_rate, drop=drop_rate,\n",
    "                                           cls_attn=cls_attn))\n",
    "\n",
    "        self.block = nn.ModuleList(tmp)\n",
    "        self.output_channels = output_channels\n",
    "        self.ws = kernel_size\n",
    "        if not isinstance(kernel_size, (tuple, list)):\n",
    "            kernel_size = [(kernel_size, kernel_size), (kernel_size, kernel_size), 0]\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.peg = nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1, groups=output_channels, bias=False) if peg else None\n",
    "\n",
    "    def forward(self, xs):\n",
    "        cls_tokens, patch_tokens = xs\n",
    "        cls_tokens, patch_tokens = self.block[0]((cls_tokens, patch_tokens))\n",
    "        out, mask, p_l, p_r, p_t, p_b, B, C, H, W = convert_to_flatten_layout(cls_tokens, patch_tokens, self.ws)\n",
    "        for i in range(1, len(self.block)):\n",
    "            blk = self.block[i]\n",
    "\n",
    "            out = blk((out, B, H, W, mask))\n",
    "            if self.peg is not None and i == 1:\n",
    "                cls_tokens, patch_tokens = convert_to_spatial_layout(out, self.output_channels, B, H, W, self.kernel_size, mask, p_l, p_r, p_t, p_b)\n",
    "                cls_tokens = cls_tokens + self.peg(cls_tokens)\n",
    "                patch_tokens = patch_tokens + self.peg(patch_tokens)\n",
    "                out, mask, p_l, p_r, p_t, p_b, B, C, H, W = convert_to_flatten_layout(cls_tokens, patch_tokens, self.ws)\n",
    "\n",
    "        cls_tokens, patch_tokens = convert_to_spatial_layout(out, self.output_channels, B, H, W, self.kernel_size, mask, p_l, p_r, p_t, p_b)\n",
    "        return cls_tokens, patch_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, act_layer, mode='sc'):\n",
    "        super().__init__()\n",
    "        tmp = []\n",
    "        #adds stride and padding to the kernel according to the description \n",
    "        if 'c' in mode:\n",
    "            ks = 2 if 's' in mode else 1\n",
    "            if ks == 2:\n",
    "                stride = ks\n",
    "                ks = ks + 1\n",
    "                padding = ks // 2\n",
    "            else:\n",
    "                stride = ks\n",
    "                padding = 0\n",
    "\n",
    "            if input_channels == output_channels and ks == 1:\n",
    "                tmp.append(nn.Identity())\n",
    "            else:\n",
    "                tmp.extend([\n",
    "                    LayerNorm2d(input_channels),\n",
    "                    act_layer(),\n",
    "                ])\n",
    "                tmp.append(nn.Conv2d(in_channels=input_channels, out_channels=output_channels, kernel_size=ks, stride=stride, padding=padding, groups=input_channels))\n",
    "\n",
    "        self.proj = nn.Sequential(*tmp)\n",
    "        self.proj_cls = self.proj\n",
    "\n",
    "    def forward(self, xs):\n",
    "        cls_tokens, patch_tokens = xs\n",
    "        # x: BxCxHxW\n",
    "        cls_tokens = self.proj_cls(cls_tokens)\n",
    "        patch_tokens = self.proj(patch_tokens)\n",
    "        return cls_tokens, patch_tokens\n",
    "\n",
    "\n",
    "def convert_to_flatten_layout(cls_tokens, patch_tokens, ws):\n",
    "    # padding if needed, and all paddings are happened at bottom and right.\n",
    "    B, C, H, W = patch_tokens.shape\n",
    "    _, _, H_ks, W_ks = cls_tokens.shape\n",
    "    need_mask = False\n",
    "    p_l, p_r, p_t, p_b = 0, 0, 0, 0\n",
    "    if H % (H_ks * ws) != 0 or W % (W_ks * ws) != 0:\n",
    "        p_l, p_r = 0, W_ks * ws - W\n",
    "        p_t, p_b = 0, H_ks * ws - H\n",
    "        patch_tokens = F.pad(patch_tokens, (p_l, p_r, p_t, p_b))\n",
    "        need_mask = True\n",
    "\n",
    "    B, C, H, W = patch_tokens.shape\n",
    "    kernel_size = (H // H_ks, W // W_ks)\n",
    "    tmp = F.unfold(patch_tokens, kernel_size=kernel_size, stride=kernel_size, padding=(0, 0))  \n",
    "    patch_tokens = tmp.transpose(1, 2).reshape(-1, C, kernel_size[0] * kernel_size[1]).transpose(-2, -1)  \n",
    "    # If the image is not in correct size then we add 0's to make it same and we have to make sure that the kernel does not use those values so this \n",
    "    #makes all the positions where 0's were added to 0 in keernel as well. \n",
    "    if need_mask:\n",
    "        BH_sK_s, ksks, C = patch_tokens.shape\n",
    "        H_s, W_s = H // ws, W // ws\n",
    "        mask = torch.ones(BH_sK_s // B, 1 + ksks, 1 + ksks, device=patch_tokens.device, dtype=torch.float)\n",
    "        right = torch.zeros(1 + ksks, 1 + ksks, device=patch_tokens.device, dtype=torch.float)\n",
    "        tmp = torch.zeros(ws, ws, device=patch_tokens.device, dtype=torch.float)\n",
    "        tmp[0:(ws - p_r), 0:(ws - p_r)] = 1.\n",
    "        tmp = tmp.repeat(ws, ws)\n",
    "        right[1:, 1:] = tmp\n",
    "        right[0, 0] = 1\n",
    "        right[0, 1:] = torch.tensor([1.] * (ws - p_r) + [0.] * p_r).repeat(ws).to(right.device)\n",
    "        right[1:, 0] = torch.tensor([1.] * (ws - p_r) + [0.] * p_r).repeat(ws).to(right.device)\n",
    "        bottom = torch.zeros_like(right)\n",
    "        bottom[0:ws * (ws - p_b) + 1, 0:ws * (ws - p_b) + 1] = 1.\n",
    "        bottom_right = copy.deepcopy(right)\n",
    "        bottom_right[0:ws * (ws - p_b) + 1, 0:ws * (ws - p_b) + 1] = 1.\n",
    "\n",
    "        mask[W_s - 1:(H_s - 1) * W_s:W_s, ...] = right\n",
    "        mask[(H_s - 1) * W_s:, ...] = bottom\n",
    "        mask[-1, ...] = bottom_right\n",
    "        mask = mask.repeat(B, 1, 1)\n",
    "    else:\n",
    "        mask = None\n",
    "\n",
    "    cls_tokens = cls_tokens.flatten(2).transpose(-2, -1)  # (N)x(H/sxK/s)xC\n",
    "    cls_tokens = cls_tokens.reshape(-1, 1, cls_tokens.size(-1))  # (NxH/sxK/s)x1xC\n",
    "\n",
    "    out = torch.cat((cls_tokens, patch_tokens), dim=1)\n",
    "\n",
    "    return out, mask, p_l, p_r, p_t, p_b, B, C, H, W\n",
    "\n",
    "\n",
    "def convert_to_spatial_layout(out, output_channels, B, H, W, kernel_size, mask, p_l, p_r, p_t, p_b):\n",
    "    \"\"\"\n",
    "    Convert the token layer from flatten into 2-D, will be used to downsample the spatial dimension.\n",
    "    \"\"\"\n",
    "    cls_tokens = out[:, 0:1, ...]\n",
    "    patch_tokens = out[:, 1:, ...]\n",
    "    # cls_tokens: (BxH/sxW/s)x(1)xC, patch_tokens: (BxH/sxW/s)x(ksxks)xC\n",
    "    C = output_channels\n",
    "    kernel_size = kernel_size[0]\n",
    "    H_ks = H // kernel_size[0]\n",
    "    W_ks = W // kernel_size[1]\n",
    "    # reorganize data, need to convert back to cls_tokens: BxCxH/sxW/s, patch_tokens: BxCxHxW\n",
    "    cls_tokens = cls_tokens.reshape(B, -1, C).transpose(-2, -1).reshape(B, C, H_ks, W_ks)\n",
    "    patch_tokens = patch_tokens.transpose(1, 2).reshape((B, -1, kernel_size[0] * kernel_size[1] * C)).transpose(1, 2)\n",
    "    patch_tokens = F.fold(patch_tokens, (H, W), kernel_size=kernel_size, stride=kernel_size, padding=(0, 0))\n",
    "\n",
    "    if mask is not None:\n",
    "        if p_b > 0:\n",
    "            patch_tokens = patch_tokens[:, :, :-p_b, :]\n",
    "        if p_r > 0:\n",
    "            patch_tokens = patch_tokens[:, :, :, :-p_r]\n",
    "\n",
    "    return cls_tokens, patch_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
